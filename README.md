# Mini-GPT
A character-level Transformer-based language model trained on Shakespeare's text.
<br>
Implements a Bigram model with self-attention using PyTorch, featuring multi-head attention, transformer blocks, and autoregressive text generation.
<br>
**Features:**
Transformer-based architecture with multi-head self-attention.
<br>
Trained on Tiny Shakespeare dataset.
<br>
Supports autoregressive text generation.
<br>
Implemented from scratch using PyTorch.
<br>
Uses AdamW optimizer for training.
