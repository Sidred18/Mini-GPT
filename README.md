# Mini-GPT
A character-level Transformer-based language model trained on Shakespeare's text.
<br>
Implements a Bigram model with self-attention using PyTorch, featuring multi-head attention, transformer blocks, and autoregressive text generation.
<br>
**Features:**
<ul>
  <li>Transformer-based architecture with multi-head self-attention.</li>
  <li>Trained on Tiny Shakespeare dataset.</li>
  <li>Supports autoregressive text generation.</li>
  <li>Implemented from scratch using PyTorch.</li>
  <li>Uses AdamW optimizer for training.</li>
</ul>

